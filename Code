import torch
import skfuzzy as fuzz
from networkx import Graph
import scipy.cluster.hierarchy as sch
import numpy as np
import torch.nn.functional as F

class FractalPerceptron:
    def __init__(self, fuzzy_network, depth):
        self.fuzzy_network = fuzzy_network
        self.depth = depth
        self.children = []
        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01)

    def forward(self, x):
        if self.depth == 1:
            return self.fuzzy_network(x)
        else:
            out = 0
            for child in self.children:
                out += child(x)
            return out

    def backward(self, grad_output):
        if self.depth == 1:
            self.fuzzy_network.backward(grad_output)
        else:
            grad_input = 0
            for child in self.children:
                child.backward(grad_output)
                grad_input += child.grad_input
            self.grad_input = grad_input

    def add_child(self, child):
        self.children.append(child)

    def parameters(self):
        if self.depth == 1:
            return self.fuzzy_network.parameters()
        else:
            return [param for child in self.children for param in child.parameters()]

class FuzzyNetwork:
    def __init__(self, input_size, output_size, num_rules):
        self.input_size = input_size
        self.output_size = output_size
        self.num_rules = num_rules
        self.input_mfs = []
        self.output_mfs = []
        self.rules = []
        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01)

        # Initialize input and output membership functions
        for i in range(input_size):
            self.input_mfs.append(fuzz.gaussmf(np.linspace(0, 1, num_rules), i/float(input_size), 0.1))
        for i in range(output_size):
            self.output_mfs.append(fuzz.gaussmf(np.linspace(0, 1, num_rules), i/float(output_size), 0.1))

        # Initialize rules
        for i in range(num_rules):
            for j in range(num_rules):
                rule = torch.tensor(np.fmin(self.input_mfs[i], self.input_mfs[j]), dtype=torch.float32)
                self.rules.append(rule)

    def forward(self, x):
        x = torch.tensor(x, dtype=torch.float32)
        inputs = []
        for i in range(self.input_size):
            inputs.append(self.input_mfs[i][x[i]])
        inputs = torch.tensor(inputs, dtype=torch.float32)
        outputs = torch.matmul(inputs, torch.stack(self.rules)).reshape(self.output_size, self.num_rules)
        outputs = torch.stack([torch.fmax(outputs[i], self.output_mfs[i]) for i in range(self.output_size)])
        outputs = torch.mean(outputs, dim=1)
        return outputs

    def backward(self, grad_output):
      grad_output = torch.tensor(grad_output, dtype=torch.float32)
      grad_input = torch.matmul(torch.stack(self.rules), grad_output.reshape(self.num_rules, 1)).reshape(self.num_rules*self.input_size)
      for i in range(self.input_size):
          grad_mf = torch.tensor(np.fmax(self.input_mfs[i], 1-self.input_mfs[i]), dtype=torch.float32)
          grad_mf[self.input_mfs[i] > 0.5] = 1 - grad_mf[self.input_mfs[i] > 0.5]
          grad_mf = grad_mf / grad_mf.sum()
          self.input_mfs[i] -= self.optimizer.param_groups[0]['lr'] * grad_input[i*self.num_rules:(i+1)*self.num_rules]
          grad_mf = grad_mf.unsqueeze(1)
          grad_rules = grad_output * grad_mf
          self.rules -= self.optimizer.param_groups[0]['lr'] * grad_rules.T                                                              
class FractalAutoML:
    def __init__(self, input_size, output_size, num_rules, fractal_depth):
          super(FractalAutoML, self).__init__()
          self.input_size = input_size
          self.output_size = output_size
          self.num_rules = num_rules
          self.fractal_depth = fractal_depth
          self.fuzzy_network = FuzzyNetwork(input_size, output_size, num_rules)
          self.fractal_perceptrons = []
          for depth in range(1, fractal_depth+1):
              self.fractal_perceptrons.append(FractalPerceptron(self.fuzzy_network, depth))

    def forward(self, x):
          out = 0
          for fractal_perceptron in self.fractal_perceptrons:
              out += fractal_perceptron(x)
          return out

    def train(self, train_data, epochs=10):
        for epoch in range(epochs):
            # Forward pass
            outputs = self(train_data)
            # Compute loss
            loss = torch.mean((outputs - train_labels)**2)
            # Backpropagate
            loss.backward()
            # Update parameters
            self.optimizer.step()
            # Print loss
            print(loss.item())

    def evaluate(self, test_data):
        outputs = self(test_data)
        loss = torch.mean((outputs - test_labels)**2)
        return loss

    def compress(self):
        # Get the teacher model
        teacher = FractalAutoML(self.input_size, self.output_size, self.num_rules, self.fractal_depth - 1)
        # Train the teacher model
        teacher.train(train_data, epochs=10)
        # Distil the knowledge from the teacher model to the student model
        self.distill(teacher)
        return self

    def distill(student, teacher, train_loader, optimizer, device, temperature=4.0, alpha=0.5):
        student.train()

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()

            # Forward pass for teacher and student networks
            teacher_output = teacher(data)
            student_output = student(data)

            # Compute soft targets from teacher
            soft_target = F.softmax(teacher_output / temperature, dim=1)

            # Compute cross-entropy loss between student predictions and soft targets
            student_loss = F.cross_entropy(student_output, target)
            distillation_loss = F.kl_div(F.log_softmax(student_output / temperature, dim=1), soft_target, reduction='batchmean') * temperature * temperature * alpha
            loss = student_loss + distillation_loss

            # Backward pass and optimization step
            loss.backward()
            optimizer.step()

        return student

class Metagraph:
    def __init__(self):
        self.graph = Graph()

    def add_node(self, node):
        self.graph.add_node(node)

    def add_edge(self, node1, node2):
        self.graph.add_edge(node1, node2)

    def get_nodes(self):
        return self.graph.nodes()

    def get_edges(self):
        return self.graph.edges()

    def get_node_neighbors(self, node):
        return self.graph.neighbors(node)                                                                                                                                                 
class InternetOfModels:
    def __init__(self):
        self.models = []
        self.metagraph = Metagraph()

    def connect_to_model(self, model_url):
        model = FractalAutoML(model_url)
        self.models.append(model)
        self.metagraph.add_node(model)

    def train(self, data):
        for model in self.models:
            model.train(data)
        self.metagraph.update_edges()

    def predict(self, input_data):
        predictions = []
        for model in self.models:
            predictions.append(model.predict(input_data))
        return predictions                                                                                                                                                                                 
class FractalAutoMLNetwork:
    def __init__(self, models, reward_func):
        self.models = models
        self.reward_func = reward_func
        self.edges = []
        
    def add_edge(self, source_idx, target_idx, weight):
        self.edges.append((source_idx, target_idx, weight))
        
    def evaluate(self, inputs):
        outputs = []
        for model in self.models:
            outputs.append(model.evaluate(inputs))
        
        rewards = []
        for i, output in enumerate(outputs):
            rewards.append(self.reward_func(output))
            
        for source_idx, target_idx, weight in self.edges:
            if rewards[source_idx] > 0:
                self.models[target_idx].train(inputs, outputs[source_idx], weight)
            elif rewards[source_idx] < 0:
                self.models[source_idx].train(inputs, outputs[target_idx], weight)
        
        return outputs                                                                                                                                                                                         
class Protocol:
    def __init__(self):
        self.inputs = []
        self.outputs = []
        self.positive_reinforcement = []
        self.negative_reinforcement = []

    def send_input(self, input_data):
        self.inputs.append(input_data)

    def receive_input(self):
        if len(self.inputs) == 0:
            return None
        return self.inputs.pop(0)

    def send_output(self, output_data):
        self.outputs.append(output_data)

    def receive_output(self):
        if len(self.outputs) == 0:
            return None
        return self.outputs.pop(0)

    def send_positive_reinforcement(self, reinforcement_data):
        self.positive_reinforcement.append(reinforcement_data)

    def receive_positive_reinforcement(self):
        if len(self.positive_reinforcement) == 0:
            return None
        return self.positive_reinforcement.pop(0)

    def send_negative_reinforcement(self, reinforcement_data):
        self.negative_reinforcement.append(reinforcement_data)

    def receive_negative_reinforcement(self):
        if len(self.negative_reinforcement) == 0:
            return None
        return self.negative_reinforcement.pop(0)
